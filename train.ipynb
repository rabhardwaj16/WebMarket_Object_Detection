{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import random\n",
    "import os\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1 = pd.read_csv('db1_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>class</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>db1.jpg</td>\n",
       "      <td>1105</td>\n",
       "      <td>919</td>\n",
       "      <td>1315</td>\n",
       "      <td>1127</td>\n",
       "      <td>object</td>\n",
       "      <td>2272</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>db1.jpg</td>\n",
       "      <td>1005</td>\n",
       "      <td>1271</td>\n",
       "      <td>1166</td>\n",
       "      <td>1363</td>\n",
       "      <td>object</td>\n",
       "      <td>2272</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>db1.jpg</td>\n",
       "      <td>600</td>\n",
       "      <td>1365</td>\n",
       "      <td>745</td>\n",
       "      <td>1494</td>\n",
       "      <td>object</td>\n",
       "      <td>2272</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>db1.jpg</td>\n",
       "      <td>945</td>\n",
       "      <td>1370</td>\n",
       "      <td>1123</td>\n",
       "      <td>1508</td>\n",
       "      <td>object</td>\n",
       "      <td>2272</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>db1.jpg</td>\n",
       "      <td>1536</td>\n",
       "      <td>1432</td>\n",
       "      <td>1898</td>\n",
       "      <td>1522</td>\n",
       "      <td>object</td>\n",
       "      <td>2272</td>\n",
       "      <td>1704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  xmin  ymin  xmax  ymax   class  width  height\n",
       "0  db1.jpg  1105   919  1315  1127  object   2272    1704\n",
       "1  db1.jpg  1005  1271  1166  1363  object   2272    1704\n",
       "2  db1.jpg   600  1365   745  1494  object   2272    1704\n",
       "3  db1.jpg   945  1370  1123  1508  object   2272    1704\n",
       "4  db1.jpg  1536  1432  1898  1522  object   2272    1704"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db1['class'].nunique(),db1['width'].nunique(),db1['height'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1['class_en'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Formula for converting pixel annotations to YOLO annotations\n",
    "\n",
    "``` python\n",
    "center_x = (x_min + x_max) / (2.0 * image_width)\n",
    "center_y = (y_min + y_max) / (2.0 * image_height)\n",
    "width = (x_max - x_min) / image_width\n",
    "height = (y_max - y_min) / image_height\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db1.rename(columns={'width':'image_width','height':'image_height'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting to YOLO annotations\n",
    "\n",
    "db1['center_x'] = (db1['xmin'] + db1['xmax']) / (2*db1['image_width'])\n",
    "db1['center_y'] = (db1['ymin'] + db1['ymax']) / (2*db1['image_height'])\n",
    "db1['width'] = (db1['xmax'] - db1['xmin']) / db1['image_width']\n",
    "db1['height'] = (db1['ymax'] - db1['ymin']) / db1['image_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in db1['filename'].unique():\n",
    "    db1[db1['filename']==file][['class_en','center_x','center_y','width','height']].to_csv('./data/labels/train/' + file[:-4] + '.txt', index=False, header=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "and\n",
    "image_item_names = [v[:-4] for v in db1['filename'].unique()]\n",
    "val_items = random.sample(image_item_names,int(.1*len(image_item_names)))\n",
    "test_items = random.sample(val_items,int(len(val_items)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a\n",
    "for item in val_items:\n",
    "    shutil.move('./data/images/train/' + item + '.jpg', './data/images/val/')\n",
    "    shutil.move('./data/labels/train/' + item + '.txt', './data/labels/val/')\n",
    "\n",
    "for item in test_items:\n",
    "    shutil.move('./data/images/val/' + item + '.jpg', './data/images/test/')\n",
    "    shutil.move('./data/labels/val/' + item + '.txt', './data/labels/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('yolov8n.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.198 ðŸš€ Python-3.8.10 torch-2.1.0 MPS (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=object_det.yaml, epochs=10, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=mps, workers=8, project=None, name=train9, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011043 parameters, 3011027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 355/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/labels/train.cache... 270 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/labels/val.cache... 15 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<?, ?it/s]\n",
      "Plotting labels to runs/detect/train9/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train9\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/10         0G      1.668      2.813      1.382        485        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:09<00:00, 14.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.20s/it]\n",
      "                   all         15        918      0.219      0.667      0.397      0.176\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/10         0G       1.48      1.872      1.174        736        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:43<00:00, 16.70s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.92s/it]\n",
      "                   all         15        918      0.424      0.407      0.337      0.154\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/10         0G       1.46      1.685      1.127        720        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:56<00:00, 17.47s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.54s/it]\n",
      "                   all         15        918      0.553      0.649      0.522      0.238\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/10         0G      1.455      1.606      1.115        520        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:47<00:00, 16.93s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.95s/it]\n",
      "                   all         15        918      0.606      0.682      0.577      0.275\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/10         0G       1.41      1.579      1.107        765        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:41<00:00, 16.54s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:08<00:00,  8.70s/it]\n",
      "                   all         15        918      0.622      0.687       0.57       0.27\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/10         0G        1.4       1.56        1.1        659        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [04:42<00:00, 16.60s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.75s/it]\n",
      "                   all         15        918      0.712      0.682      0.614      0.282\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/10         0G        1.4      1.503      1.084        522        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [05:10<00:00, 18.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.98s/it]\n",
      "                   all         15        918      0.626      0.694      0.574      0.269\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/10         0G      1.373      1.475      1.063        487        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [05:52<00:00, 20.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:09<00:00,  9.90s/it]\n",
      "                   all         15        918      0.716      0.729      0.649      0.314\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/10         0G      1.344      1.478      1.057        686        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [05:46<00:00, 20.38s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:10<00:00, 10.23s/it]\n",
      "                   all         15        918      0.663      0.687      0.595       0.29\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/10         0G      1.329      1.427      1.043        803        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [05:57<00:00, 21.00s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.91s/it]\n",
      "                   all         15        918        0.7      0.701       0.62      0.309\n",
      "\n",
      "10 epochs completed in 0.864 hours.\n",
      "Optimizer stripped from runs/detect/train9/weights/last.pt, 6.2MB\n",
      "Optimizer stripped from runs/detect/train9/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating runs/detect/train9/weights/best.pt...\n",
      "Ultralytics YOLOv8.0.198 ðŸš€ Python-3.8.10 torch-2.1.0 MPS (Apple M1 Pro)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:11<00:00, 71.57s/it]\n",
      "                   all         15        918      0.715      0.729      0.649      0.314\n",
      "Speed: 8.3ms preprocess, 22.3ms inference, 0.0ms loss, 5.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train9\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "results = model.train(data='object_det.yaml', epochs=10, device='mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./best.pt'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shutil.copy('./runs/detect/train9/weights/best.pt','./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction using CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.198 ðŸš€ Python-3.8.10 torch-2.1.0 CPU (Apple M1 Pro)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/images/train/db1.jpg: 480x640 41 objects, 47.2ms\n",
      "Speed: 2.2ms preprocess, 47.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict5\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# predicting from train dataset\n",
    "\n",
    "!yolo predict model=best.pt source='./data/images/train/db1.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.198 ðŸš€ Python-3.8.10 torch-2.1.0 CPU (Apple M1 Pro)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/images/val/db20.jpg: 480x640 58 objects, 53.1ms\n",
      "Speed: 2.9ms preprocess, 53.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict6\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# predicting from validation dataset\n",
    "\n",
    "!yolo predict model=best.pt source='./data/images/val/db20.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.198 ðŸš€ Python-3.8.10 torch-2.1.0 CPU (Apple M1 Pro)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "image 1/1 /Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/images/test/db132.jpg: 480x640 109 objects, 51.1ms\n",
      "Speed: 2.1ms preprocess, 51.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict7\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# predicting from test dataset\n",
    "\n",
    "\n",
    "!yolo predict model=best.pt source='./data/images/test/db132.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction using Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLO('best.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'/Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/images/test/db167.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/images/test/db210.jpg: 480x640 42 objects, 54.8ms\n",
      "Speed: 2.7ms preprocess, 54.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns/detect/predict8\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[ultralytics.engine.results.Results object with attributes:\n",
       " \n",
       " boxes: ultralytics.engine.results.Boxes object\n",
       " keypoints: None\n",
       " masks: None\n",
       " names: {0: 'object'}\n",
       " orig_img: array([[[227, 224, 220],\n",
       "         [227, 224, 220],\n",
       "         [229, 226, 222],\n",
       "         ...,\n",
       "         [ 14,   8,   9],\n",
       "         [ 11,   5,   6],\n",
       "         [  9,   3,   4]],\n",
       " \n",
       "        [[226, 223, 219],\n",
       "         [227, 224, 220],\n",
       "         [228, 225, 221],\n",
       "         ...,\n",
       "         [ 13,   8,   9],\n",
       "         [ 11,   6,   7],\n",
       "         [  9,   4,   5]],\n",
       " \n",
       "        [[224, 223, 219],\n",
       "         [225, 224, 220],\n",
       "         [226, 225, 221],\n",
       "         ...,\n",
       "         [ 13,   8,   9],\n",
       "         [ 12,   7,   8],\n",
       "         [ 10,   5,   6]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[111, 108,  70],\n",
       "         [143, 142, 104],\n",
       "         [176, 182, 141],\n",
       "         ...,\n",
       "         [226, 227, 225],\n",
       "         [226, 227, 225],\n",
       "         [226, 227, 225]],\n",
       " \n",
       "        [[131, 128,  90],\n",
       "         [149, 148, 110],\n",
       "         [169, 177, 136],\n",
       "         ...,\n",
       "         [227, 228, 226],\n",
       "         [227, 228, 226],\n",
       "         [227, 228, 226]],\n",
       " \n",
       "        [[167, 163, 128],\n",
       "         [168, 166, 131],\n",
       "         [170, 178, 138],\n",
       "         ...,\n",
       "         [227, 228, 226],\n",
       "         [227, 228, 226],\n",
       "         [227, 228, 226]]], dtype=uint8)\n",
       " orig_shape: (1704, 2272)\n",
       " path: '/Users/rajatbhardwaj/Downloads/WebMarket_Object_Detection/data/images/test/db210.jpg'\n",
       " probs: None\n",
       " save_dir: 'runs/detect/predict8'\n",
       " speed: {'preprocess': 2.749204635620117, 'inference': 54.7640323638916, 'postprocess': 2.1300315856933594}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict('./data/images/test/db210.jpg',save=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
